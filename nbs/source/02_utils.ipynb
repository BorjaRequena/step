{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b44dceed",
   "metadata": {},
   "source": [
    "---\n",
    "description: Utility functionalities or recurrent operations for the machine learning pipeline.\n",
    "output-file: utils.html\n",
    "title: Utils\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12964991-261a-4c36-acc6-981ae9f552f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d1278-75c9-41f0-b8b3-fbb4f225f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "import ruptures as rpt\n",
    "from tqdm.auto import tqdm\n",
    "from fastcore.all import *\n",
    "import matplotlib.colors as clr\n",
    "from fastai.metrics import F1ScoreMulti\n",
    "from fastai.torch_core import tensor, to_detach\n",
    "from step.data import DATA_PATH, get_andi_valid_dls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd266f0-7335-4316-ae4e-4462100a9f66",
   "metadata": {},
   "source": [
    "# Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a92cc7-de91-42ba-94e7-d90c63511c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lengths_from_cps(cps, length=200):\n",
    "    \"Returns segment lengths determined by `cps` and a total length `length`.\"\n",
    "    ex_cps = torch.cat((tensor([0]), cps, tensor([length])))\n",
    "    return ex_cps[1:] - ex_cps[:-1]\n",
    "\n",
    "def split_tensor(t, indices):\n",
    "    \"Splits input tensor `t` according to indices in the first dimension.\"\n",
    "    idx = [0] + list(indices) + [len(t)]\n",
    "    return [t[i:j] for i, j in zip(idx[:-1], idx[1:])]\n",
    "\n",
    "def get_displacements(x):\n",
    "    \"Returns the displacements of trajectory `x` [dim, length].\"\n",
    "    return np.sqrt(np.sum(np.diff(x, axis=1)**2, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21ec4e-cef1-47c3-b1d6-4c1e627afcdb",
   "metadata": {},
   "source": [
    "# Segmentation post-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe4bd0-0ddb-409d-999a-0bae581dcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import ruptures as rpt\n",
    "@delegates(rpt.KernelCPD)\n",
    "def fit_segments(pred, pen=1., return_cps=False, **kwargs):\n",
    "    \"Fit piecewise constant segments to input signal `pred`.\"\n",
    "    alg = rpt.KernelCPD(**kwargs).fit(pred.numpy())\n",
    "    cps = [0] + alg.predict(pen=pen)\n",
    "    seg_fit = torch.ones_like(pred)\n",
    "    for i, j in zip(cps[:-1], cps[1:]):\n",
    "        seg_fit[i:j] *= pred[i:j].mean()\n",
    "    if return_cps: return seg_fit, np.array(cps)\n",
    "    return seg_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902ff15",
   "metadata": {},
   "source": [
    "`fit_segments` is mainly intended to process predictions of continuous values. However, the following functinos are mainly intended to post-process discrete predictions. Notably, `post_process_prediction` takes a prediction of discrete categories over a trajectory and extracts the most likely changepoints and segments, minimizing the impact of spurious mistakes along the predicted segment (see the example below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_change_points(t): \n",
    "    \"Finds points in tensor `t` where the value changes.\"\n",
    "    return ((t[:-1] - t[1:]) != 0).nonzero(as_tuple=True)[0] + 1\n",
    "\n",
    "def get_splits(t): \n",
    "    \"Splits tensor `t` into chunks with the same value.\"\n",
    "    cps = find_change_points(t)\n",
    "    sizes = _find_split_sizes(t, cps)\n",
    "    return list(t.split(sizes.tolist()))\n",
    "\n",
    "def _find_split_sizes(t, change_points):\n",
    "    \"Finds sizes of chunks in `t` delimited by `change_points`.\"\n",
    "    z, max_len = torch.zeros(1, dtype=int, device=t.device), tensor([len(t)], device=t.device)\n",
    "    cps_ext = torch.cat((z, change_points, max_len))\n",
    "    return cps_ext[1:] - cps_ext[:-1]\n",
    "\n",
    "def change_points_from_splits(splits):\n",
    "    \"Returns change point position from split tensor.\"\n",
    "    return torch.cumsum(tensor([len(s) for s in splits[:-1]], device=splits[0].device), dim=0)\n",
    "        \n",
    "def get_split_classes(splits): \n",
    "    \"Returns majority class of each split.\"\n",
    "    return [majority_vote(s) for s in splits]\n",
    "\n",
    "def majority_vote(t):\n",
    "    \"Returns majoritary value from `t`.\"\n",
    "    values, counts = t.unique(return_counts=True)\n",
    "    max_idx = (counts == counts.max()).float().multinomial(1) # break ties randomly\n",
    "    return values[max_idx]\n",
    "\n",
    "def abundance(val, t):\n",
    "    \"Abundance of value `val` in tensor `t`.\"\n",
    "    vals, counts = t.unique(return_counts=True)\n",
    "    if val in vals: return counts[vals == val]/counts.sum()\n",
    "    else: return 0.\n",
    "    \n",
    "def post_process_prediction(pred, n_change_points=1):\n",
    "    \"Segmentation prediction post-processing to find change points and classes.\"\n",
    "    if len(pred.squeeze().shape) == 2: pred = pred.argmax(-1)\n",
    "    splits = get_splits(pred)\n",
    "    none_can_merge = False\n",
    "    while len(splits) > n_change_points + 1:\n",
    "        sizes = tensor([len(s) for s in splits])\n",
    "        idx_merge = (sizes[1:-1].argsort() + 1).tolist()\n",
    "        none_can_merge = True\n",
    "        for i in idx_merge:\n",
    "            if _can_merge(splits, i): \n",
    "                splits = _merge_splits(splits, i)\n",
    "                none_can_merge = False\n",
    "                break\n",
    "\n",
    "        if none_can_merge:\n",
    "            len0 = len(splits)\n",
    "            splits = _merge_contiguous_values(splits)\n",
    "            len1 = len(splits)\n",
    "            if len1 < len0: none_can_merge = False\n",
    "\n",
    "        if none_can_merge: \n",
    "            splits = _merge_edge(splits)\n",
    "            none_can_merge = False\n",
    "            \n",
    "    classes = get_split_classes(splits)\n",
    "    change_points = change_points_from_splits(splits)\n",
    "    return change_points, classes, splits\n",
    "\n",
    "def _merge_left(splits, i):\n",
    "    \"Merges split `i` to the left.\"\n",
    "    return [torch.cat(splits[k-1:k+1]) if k == i else splits[k] \n",
    "            for k in range(len(splits)) if not k == i - 1]\n",
    "\n",
    "def _merge_right(splits, i):\n",
    "    \"Merges split `i` to the right.\"\n",
    "    return [torch.cat(splits[k:k+2]) if k == i else splits[k] \n",
    "            for k in range(len(splits)) if not k == i + 1]\n",
    "\n",
    "def _merge_left_or_right(splits, i):\n",
    "    \"Merges split `i` towards left or right depending on majority classes.\"\n",
    "    left_slice, right_slice = torch.cat(splits[:i]), torch.cat(splits[i+1:])\n",
    "    classes, counts = splits[i].unique(return_counts=True)\n",
    "    for c in classes[counts.argsort(descending=True)]:\n",
    "        abundance_left, abundance_right = abundance(c, left_slice), abundance(c, right_slice)\n",
    "        if   abundance_left > abundance_right: return _merge_left(splits, i)\n",
    "        elif abundance_right > abundance_left: return _merge_right(splits, i)\n",
    "\n",
    "    if i == 1 and len(splits[i]) > len(splits[0]): return _merge_left(splits, i)\n",
    "    if i == len(splits) - 2 and len(splits[i]) > len(splits[-1]): return _merge_right(splits, i)\n",
    "    else: return _merge_left(splits, i) if torch.randint(2, (1,)) else _merge_right(splits, i)\n",
    "\n",
    "def _can_merge(splits, i):\n",
    "    \"Checks whether split `i` is suitable for merging.\"\n",
    "    classes = get_split_classes(splits)\n",
    "    return i == 0 or i == len(splits) - 1 or classes[i-1] == classes[i+1]\n",
    "\n",
    "def _merge_splits(splits, i):\n",
    "    \"Merges split `i` in `splits` with a contiguous one.\"\n",
    "    if   i == 0:               return _merge_right(splits, i)\n",
    "    elif i == len(splits) - 1: return _merge_left(splits, i)\n",
    "    else:                      return _merge_left_or_right(splits, i)\n",
    "    \n",
    "def _merge_contiguous_values(splits):\n",
    "    \"Merges contiguous splits of the same class.\"\n",
    "    classes = get_split_classes(splits)\n",
    "    max_len = len(splits)\n",
    "    for e, (c0, c1) in enumerate(zip(classes[-2::-1], classes[:0:-1])):\n",
    "        if c0 == c1: \n",
    "            idx = max_len - e - 1\n",
    "            splits = _merge_left(splits, idx)\n",
    "    return splits\n",
    "\n",
    "def _merge_edge(splits):\n",
    "    \"Merges one of the edge splits.\"\n",
    "    left, right, adj_left, adj_right = splits[0], splits[-1], splits[1], splits[-2]\n",
    "    (vl, cl), (vr, cr) = left.unique(return_counts=True), right.unique(return_counts=True)\n",
    "    idx_r = len(splits) - 1\n",
    "    sim_left  = [abundance(v, adj_left)*abundance(v, left) for v, c in zip(vl, cl)]\n",
    "    sim_right = [abundance(v, adj_right)*abundance(v, right) for v, c in zip(vr, cr)]\n",
    "    sim_left, sim_right = torch.mean(tensor(sim_left)), torch.mean(tensor(sim_right))\n",
    "\n",
    "    if   sim_left > sim_right: return _merge_right(splits, 0)\n",
    "    elif sim_right > sim_left: return _merge_left(splits, idx_r)\n",
    "        \n",
    "    if    len(left) < len(adj_left) and len(right) > len(adj_right):\n",
    "        return _merge_right(splits, 0)\n",
    "    elif  len(left) > len(adj_left) and len(right) < len(adj_right):\n",
    "        return _merge_left(splits, idx_r)\n",
    "    elif  len(left) < len(right):\n",
    "        return _merge_right(splits, 0)\n",
    "    elif  len(left) > len(right):\n",
    "        return _merge_left(splits, idx_r)\n",
    "    else:\n",
    "        return _merge_left(splits, idx_r) if torch.randint(2, (1,)) else _merge_right(splits, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95001fb9-57e7-4273-9a9f-2aaf6f17b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8]),\n",
       " [tensor([0]), tensor([2])],\n",
       " [tensor([0, 0, 0, 0, 1, 1, 0, 0]), tensor([2, 2, 2, 2, 1, 2, 2])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tensor([0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 2])\n",
    "cps, classes, splits = post_process_prediction(prediction)\n",
    "cps, classes, splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bdcc80",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6ee10-3a54-4686-9674-a5e65c3ffa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_absolute_error(pred, true):\n",
    "    \"Mean absolute error between `pred` and `true`.\"\n",
    "    return (pred - true).abs().mean()\n",
    "\n",
    "def mean_relative_error(pred, true, base=10):\n",
    "    \"Mean relative error assuming `pred` and `true` in log_base.\"\n",
    "    error = pred - true\n",
    "    return (base**error - 1).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_cp_prediction(true, pred, changepoint_threshold=5):\n",
    "    \"Evaluates the change point prediction.\"\n",
    "    true_positive = 0\n",
    "    false_positive = max(len(pred) - len(true), 0)\n",
    "    false_negative = max(len(true) - len(pred), 0)\n",
    "    squared_error = []\n",
    "    \n",
    "    assignment = assign_changepoints(true, pred)\n",
    "    for idx in assignment:\n",
    "        difference = np.abs(true[idx[0]] - pred[idx[1]])\n",
    "        if difference < changepoint_threshold:\n",
    "            true_positive += 1\n",
    "            squared_error.append(difference**2)\n",
    "        else:\n",
    "            false_positive += 1\n",
    "            false_negative += 1\n",
    "            \n",
    "    return {'squared_error': squared_error, \n",
    "            'tp': true_positive, \n",
    "            'fp': false_positive, \n",
    "            'fn': false_negative}\n",
    "\n",
    "def assign_changepoints(true, pred):\n",
    "    \"Matches predicted and true changepoints solving a linear sum assignment problem.\"\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    cost = np.zeros((len(true), len(pred)))\n",
    "    for i, t in enumerate(true):\n",
    "        cost[i, :] = np.abs(t-pred)\n",
    "    return np.array(linear_sum_assignment(cost)).T\n",
    "\n",
    "def jaccard_index(true_positive, false_positive, false_negative):\n",
    "    \"Computes the Jaccard index a.k.a. Tanimoto index.\"\n",
    "    return true_positive/(true_positive + false_positive + false_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464deb4-e414-4fde-bf44-81ac89d33ce6",
   "metadata": {},
   "source": [
    "Since the changepoint detection algorithm can provide an arbitrary number of change points, we solve a linear sum assignment problem to perform the matching between the ground truth and the predicted changepoints.\n",
    "\n",
    "Then, we consider a valid prediction, i.e., a true positive (TP), those changepoints that lie within a trheshold of their corresponding ground truth. This way, all the predicted change points that are not TP are false positives (FP). Finally, the ground truth change points that do not have a predicted counterpart within the threshold are false negatives (FN). \n",
    "\n",
    "To evaluate the change point detection, we use the Jaccard index, which is a function of the TP, FP and FN: $$JI = \\frac{TP}{TP + FP + FN}\\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06902e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eval_andi_metrics(dls, model):\n",
    "    \"Evaluates model in validation set in order to obtain AnDi challenge metrics.\"\n",
    "    f1_score = F1ScoreMulti(average='micro')\n",
    "    cps_pred, cls0_pred, cls1_pred = [], [], []\n",
    "    cps_true, cls0_true, cls1_true = [], [], []\n",
    "    for x, y in dls.valid:\n",
    "        pred = model.activation(model(x)).detach()\n",
    "        for p, true in zip(pred, y):\n",
    "            cp_p, cls_p, _ = post_process_prediction(p)\n",
    "            cp_t, cls_t, _ = post_process_prediction(true)\n",
    "            cps_true.append(cp_t[0].item()) \n",
    "            cls0_true.append(cls_t[0].item()); cls1_true.append(cls_t[1].item())\n",
    "            if len(cls_p) < 2: \n",
    "                cls0_pred.append(cls_p[0].item())\n",
    "                cls1_pred.append(cls_p[0].item())\n",
    "                cps_pred.append(0)\n",
    "            else:\n",
    "                cls0_pred.append(cls_p[0].item())\n",
    "                cls1_pred.append(cls_p[1].item())\n",
    "                cps_pred.append(cp_p[0].item())\n",
    "\n",
    "    cps_pred, cps_true = tensor(cps_pred), tensor(cps_true)\n",
    "    full_preds = torch.cat((tensor(cls0_pred), tensor(cls1_pred)), axis=0)\n",
    "    full_true = torch.cat((tensor(cls0_true), tensor(cls1_true)), axis=0)\n",
    "    \n",
    "    rmse = (cps_pred - cps_true).pow(2).float().mean().sqrt()\n",
    "    f1 = f1_score(full_preds, full_true)\n",
    "    return rmse, f1\n",
    "\n",
    "@delegates(get_andi_valid_dls)\n",
    "def validate_andi_1(m, dim=1, bs=1, **kwargs):\n",
    "    \"Validates model on the AnDi test set for task 1 (anomalous exponent).\"\n",
    "    pred_path = DATA_PATH/\"task1\"\n",
    "    dls = get_andi_valid_dls(dim=dim, task=1, bs=1, **kwargs)\n",
    "    dls.device = next(m.parameters()).device\n",
    "    preds = [to_detach(m.activation(m(x))) for x,_ in tqdm(dls.valid)]\n",
    "    with open(pred_path.with_suffix('.txt'), 'w') as f:\n",
    "        for p in preds:\n",
    "            alpha = p.mean().item()\n",
    "            #dim; alpha\n",
    "            f.write(f'{int(dim)}; {alpha}\\n')\n",
    "\n",
    "@delegates(get_andi_valid_dls)\n",
    "def validate_andi_3_models(m, dim=1, task=3, **kwargs):\n",
    "    \"Validates model on the AnDi test set for task 3 (segmentation) predicting diffusion models.\"\n",
    "    pred_path = DATA_PATH/\"task3\"\n",
    "    dls = get_andi_valid_dls(dim=dim, task=3, **kwargs)\n",
    "    dls.device = next(m.parameters()).device\n",
    "    preds = torch.cat([to_detach(m.activation(m(x))) for x,_ in tqdm(dls.valid)])\n",
    "    with open(pred_path.with_suffix('.txt'), 'w') as f:\n",
    "        for p in preds:\n",
    "            cp, classes, _ = post_process_prediction(p)\n",
    "            if len(classes) < 2: \n",
    "                cp = tensor(100)\n",
    "                classes.append(classes[0])\n",
    "            #dim; cp; model_0; alpha_0; model_1; alpha_1\n",
    "            f.write(f'{int(dim)}; {cp.item()}; {classes[0].item()}; 0.; {classes[1].item()}; 0.\\n')\n",
    "            \n",
    "@delegates(get_andi_valid_dls)\n",
    "def validate_andi_3_alpha(m, dim=1, task=3, **kwargs):\n",
    "    \"Validates model on the AnDi test set for task 3 (segmentation) predicting anomalous exponents.\"\n",
    "    pred_path = DATA_PATH/\"task3\"\n",
    "    dls = get_andi_valid_dls(dim=dim, task=3, **kwargs)\n",
    "    dls.device = next(m.parameters()).device\n",
    "    preds = torch.cat([to_detach(m.activation(m(x))) for x,_ in tqdm(dls.valid)])\n",
    "    with open(pred_path.with_suffix('.txt'), 'w') as f:\n",
    "        for p in preds:\n",
    "            cp = rpt.KernelCPD(min_size=5).fit(p.numpy()).predict(n_bkps=1)[0]\n",
    "            alpha_0, alpha_1 = p[:cp].mean(), p[cp:].mean()\n",
    "            #dim; cp; model_0; alpha_0; model_1; alpha_1\n",
    "            f.write(f'{int(dim)}; {cp}; 0; {alpha_0}; 0; {alpha_1}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd81451-aa79-4c77-beed-6ee6d1c921ae",
   "metadata": {},
   "source": [
    "# Figures\n",
    "\n",
    "Here, we define colors and colormaps for our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aaf715-9c13-441d-86d5-f15108980a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "color_order = ['blue', 'orange', 'yellow', 'purple', 'green']\n",
    "color_dict = {\n",
    "    'blue':   {'dark': (0.2745098, 0.4, 0.6),\n",
    "               'medium': (0.39607843, 0.5254902, 0.71764706),\n",
    "               'light': (0.65098039, 0.79215686, 0.94117647)},\n",
    "    'orange': {'dark': (0.71764706, 0.36470588, 0.24313725),\n",
    "               'medium': (0.88627451, 0.4627451, 0.34901961),\n",
    "               'light': (1.0, 0.63921569, 0.44705882)},\n",
    "    'yellow': {'dark': (0.85882353, 0.58431373, 0.18039216),\n",
    "               'medium': (0.89803922, 0.68235294, 0.39607843),\n",
    "               'light': (0.96470588, 0.84705882, 0.52941176)},\n",
    "    'purple': {'dark': (0.6627451, 0.16078431, 0.30980392),\n",
    "               'medium': (0.7372549, 0.39607843, 0.55294118),\n",
    "               'light': (0.89019608, 0.38823529, 0.52941176)},\n",
    "    'green':  {'dark': (0.22352941, 0.46666667, 0.4549019607843137),\n",
    "               'medium': (0.29803922, 0.60784314, 0.58431373),\n",
    "               'light': (0.50980392, 0.76862745, 0.76470588)}\n",
    "}\n",
    "\n",
    "colors = [color_dict[k]['medium'] for k in color_order]\n",
    "colors_light = [color_dict[k]['light'] for k in color_order]\n",
    "colors_dark = [color_dict[k]['dark'] for k in color_order]\n",
    "\n",
    "cmap_hist1 = clr.LinearSegmentedColormap.from_list(\n",
    "    'custom cm', ['w', \n",
    "                  color_dict['blue']['light'],\n",
    "                  color_dict['blue']['dark']],\n",
    "                  N=256\n",
    ")\n",
    "cmap_hist2 = clr.LinearSegmentedColormap.from_list(\n",
    "    'custom cm', ['w', \n",
    "                  color_dict['orange']['light'],\n",
    "                  color_dict['orange']['dark']],\n",
    "                  N=256\n",
    ")\n",
    "cmap_points = clr.LinearSegmentedColormap.from_list(\n",
    "    'custom cm', [color_dict['yellow']['light'], \n",
    "                  color_dict['purple']['light'],\n",
    "                  color_dict['blue']['medium']],\n",
    "                  N=256\n",
    ")\n",
    "\n",
    "fig_size = 4\n",
    "linewidth = 2\n",
    "alpha_grid = 0.2\n",
    "scatter_size = 12\n",
    "\n",
    "D_units = \"($\\mu$m$^2$/s)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
